{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.Network import Face_Detection_Model, rgbdp_v2_twostep_model, rgbdp_v3_twostep_model\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T \n",
    "import os\n",
    "import os.path as osp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_model = Face_Detection_Model(3).to('cuda:0').eval()\n",
    "lidar_model = rgbdp_v3_twostep_model(device='cuda:0').eval()\n",
    "cloudnet_model = rgbdp_v2_twostep_model(device='cuda:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_rgb = torch.cuda.FloatTensor(4, 3, 180, 180)\n",
    "dummy_depth = torch.cuda.FloatTensor(4, 1, 180, 180)\n",
    "dummy_cloud = torch.cuda.FloatTensor(4, 3, 180, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13044131]\n"
     ]
    }
   ],
   "source": [
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "    \n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = rgb_model(dummy_rgb)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "print(sum(timings)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13834649]\n"
     ]
    }
   ],
   "source": [
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "    \n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = lidar_model(dummy_rgb, dummy_depth, dummy_cloud)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "print(sum(timings)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.27154058]\n"
     ]
    }
   ],
   "source": [
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "    \n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = cloudnet_model(dummy_rgb, dummy_depth, dummy_cloud)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "print(sum(timings)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. cpu / gpu 둘다 해볼 것\n",
    "모바일넷 논문 참고할 것. \n",
    "\n",
    "2. gpu 를 스펙 낮은 gpu 에서. \n",
    "\n",
    "3. 모든 걸 할 땐 batch 사이즈 1개, gpu 1개 사용할 것. \n",
    "\n",
    "4. .py 파일만들고, 리눅스 상에서 타임체크할 것. \n",
    "서버 환경까지 적어줘야 함. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_Data(Dataset):\n",
    "\n",
    "    def __init__(self, data_paths, crop=False):\n",
    "        self.data_paths = data_paths\n",
    "        self.crop = crop\n",
    "        \n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "        self.transforms = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            normalize\n",
    "        ])   \n",
    "        self.transforms2 = T.Compose([\n",
    "            T.ToTensor()\n",
    "        ])               \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        rgb_path = self.data_paths[index][0]\n",
    "        cloud_path = self.data_paths[index][1]\n",
    "        depth_path = self.data_paths[index][2]   \n",
    "             \n",
    "        # crop setting\n",
    "        crop_width = 90\n",
    "        crop_height = 150\n",
    "        mid_x, mid_y = 90, 90\n",
    "        offset_x, offset_y = crop_width//2, crop_height//2\n",
    "        \n",
    "        # RGB open and crop \n",
    "        rgb_data = cv2.imread(rgb_path)\n",
    "        rgb_data = cv2.cvtColor(rgb_data, cv2.COLOR_BGR2RGB)\n",
    "        rgb_data = cv2.resize(rgb_data, (180,180), interpolation=cv2.INTER_CUBIC)\n",
    "        if self.crop == True:\n",
    "            rgb_data = rgb_data[mid_y-offset_y:mid_y+offset_y, mid_x-offset_x:mid_x+offset_x]     \n",
    "        if self.transforms is not None :\n",
    "            rgb_data = self.transforms(rgb_data)\n",
    "            \n",
    "        # Depth open \n",
    "        depth_data = cv2.imread(depth_path)\n",
    "        depth_data = cv2.cvtColor(depth_data, cv2.COLOR_BGR2GRAY)\n",
    "        depth_data = cv2.resize(depth_data, (180,180), interpolation=cv2.INTER_CUBIC)        \n",
    "        if self.transforms2 is not None :\n",
    "            depth_data = self.transforms2(depth_data)            \n",
    "            \n",
    "        # Point Cloud(192, 256, 3) open and crop \n",
    "        cloud_data = np.load(cloud_path)\n",
    "        cloud_data = cv2.resize(cloud_data, (180,180), interpolation=cv2.INTER_CUBIC)\n",
    "        cloud_data += 5\n",
    "        if self.crop == True:\n",
    "            cloud_data = cloud_data[mid_y-offset_y:mid_y+offset_y, mid_x-offset_x:mid_x+offset_x]\n",
    "        \n",
    "        # Point Cloud and Depth Scaling\n",
    "        shift_value = 0\n",
    "        xcoor = np.array(cloud_data[:, :, 0] + shift_value)\n",
    "        ycoor = np.array(cloud_data[:, :, 1] + shift_value)\n",
    "        zcoor = np.array(cloud_data[:, :, 2] + shift_value)\n",
    "        # depth = np.array(cloud_data[:, :, 3] + shift_value)  \n",
    "        \n",
    "        # Min Max         \n",
    "        xcoor = (xcoor-xcoor.min())/(xcoor.max()-xcoor.min())\n",
    "        ycoor = (ycoor-ycoor.min())/(ycoor.max()-ycoor.min())\n",
    "        zcoor = (zcoor-zcoor.min())/(zcoor.max()-zcoor.min())\n",
    "        # depth = (depth-depth.min())/(depth.max()-depth.min())  \n",
    "        \n",
    "        scaled_cloud_data = np.concatenate([xcoor[np.newaxis,:],ycoor[np.newaxis,:],zcoor[np.newaxis,:]]) \n",
    "        # scaled_depth_data = depth[np.newaxis,:]\n",
    "        \n",
    "        # label - { 0 : real , 1 : mask }\n",
    "        if 'bonafide' in rgb_path :\n",
    "            label = 0\n",
    "        elif 'attack_mask' in rgb_path :\n",
    "            label = 1\n",
    "        elif 'attack_replay' in rgb_path :\n",
    "            label = 1\n",
    "        elif 'attack_paper' in rgb_path :\n",
    "            label = 1\n",
    "        # return rgb_data, scaled_cloud_data, scaled_depth_data, label\n",
    "        return rgb_data, scaled_cloud_data, depth_data, label\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(): \n",
    "        \n",
    "    ## Input : RGB(3-channel) + Depth(1-channel) + Point_Cloud(3-channel)\n",
    "    data_path = '/mnt/nas3/yrkim/liveness_lidar_project/GC_project/LDFAS/1. Indoor'\n",
    "    npy_path = '/mnt/nas3/yrkim/liveness_lidar_project/GC_project/LDFAS/NPY_Files/1. Indoor'\n",
    "\n",
    "    traindata_count = [1]\n",
    " \n",
    "    train_img_paths = []\n",
    "    for i in traindata_count :\n",
    "        img_path = osp.join(data_path, str(i), 'bonafide')\n",
    "        files = os.listdir(img_path)\n",
    "        rgbs = [j for j in files if (j.split('.')[-1]=='jpg') and (j.split('_')[0]=='rgb')]\n",
    "        depths = [j for j in files if (j.split('.')[-1]=='jpg') and (j.split('_')[0]=='depth')]\n",
    "        \n",
    "        # RGB \n",
    "        bonafide_files = [osp.join(data_path, str(i), 'bonafide', j) for j in rgbs]\n",
    "        paper_files= [osp.join(data_path, str(i), 'attack_paper', j) for j in rgbs]\n",
    "        replay_files= [osp.join(data_path, str(i), 'attack_replay', j) for j in rgbs]\n",
    "        mask_files= [osp.join(data_path, str(i), 'attack_mask', j) for j in rgbs]\n",
    "        \n",
    "        # Depth\n",
    "        bonafide_depths = [osp.join(data_path, str(i), 'bonafide', j) for j in depths]\n",
    "        paper_depths= [osp.join(data_path, str(i), 'attack_paper', j) for j in depths]\n",
    "        replay_depths= [osp.join(data_path, str(i), 'attack_replay', j) for j in depths]\n",
    "        mask_depths= [osp.join(data_path, str(i), 'attack_mask', j) for j in depths]       \n",
    "        \n",
    "        # Point Cloud\n",
    "        bonafide_cloud_files = [osp.join(npy_path, 'real_cloud_data',j.split('/')[-3], \n",
    "                                (('pc_'+j.split('/')[-1].split('_')[-1]).split('.')[0]+'.npy')) for j in bonafide_files]\n",
    "        paper_cloud_files = [osp.join(npy_path, 'paper_cloud_data',j.split('/')[-3], \n",
    "                                (('pc_'+j.split('/')[-1].split('_')[-1]).split('.')[0]+'.npy')) for j in paper_files]\n",
    "        replay_cloud_files = [osp.join(npy_path, 'replay_cloud_data',j.split('/')[-3], \n",
    "                                (('pc_'+j.split('/')[-1].split('_')[-1]).split('.')[0]+'.npy')) for j in replay_files]\n",
    "        mask_cloud_files = [osp.join(npy_path, 'mask_cloud_data',j.split('/')[-3], \n",
    "                                (('pc_'+j.split('/')[-1].split('_')[-1]).split('.')[0]+'.npy')) for j in mask_files]\n",
    "        \n",
    "        # bonafide\n",
    "        train_img_paths += list(zip(bonafide_files,bonafide_cloud_files,bonafide_depths))[:]\n",
    "        \n",
    "        # PAs\n",
    "        train_img_paths += list(zip(paper_files,paper_cloud_files,paper_depths))[:]\n",
    "        train_img_paths += list(zip(replay_files,replay_cloud_files,replay_depths))[:]\n",
    "        train_img_paths += list(zip(mask_files,mask_cloud_files,mask_depths))[:]\n",
    " \n",
    "    train_dataset=Face_Data(train_img_paths, False)\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset()\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, data in enumerate(train_loader) :  \n",
    "    rgb, cloud, depth, label = data\n",
    "    rgb = rgb.float().to('cuda:0') \n",
    "    cloud = cloud.float().to('cuda:0') \n",
    "    depth = depth.float().to('cuda:0') \n",
    "    label = label.float().to('cuda:0') \n",
    "\n",
    "    _ = rgb_model(rgb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, data in enumerate(train_loader) :  \n",
    "    rgb, cloud, depth, label = data\n",
    "    rgb = rgb.float().to('cuda:0') \n",
    "    cloud = cloud.float().to('cuda:0') \n",
    "    depth = depth.float().to('cuda:0') \n",
    "    label = label.float().to('cuda:0') \n",
    "\n",
    "    _ = lidar_model(rgb, depth, cloud) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, data in enumerate(train_loader) :  \n",
    "    rgb, cloud, depth, label = data\n",
    "    rgb = rgb.float().to('cuda:0') \n",
    "    cloud = cloud.float().to('cuda:0') \n",
    "    depth = depth.float().to('cuda:0') \n",
    "    label = label.float().to('cuda:0') \n",
    "\n",
    "    _ = cloudnet_model(rgb, depth, cloud) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.Network import Face_Detection_Model, rgbdp_v2_twostep_model, rgbdp_v3_twostep_model\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T \n",
    "import os\n",
    "import os.path as osp\n",
    "import cv2\n",
    "\n",
    "class Face_Data(Dataset):\n",
    "\n",
    "    def __init__(self, data_paths, crop=False):\n",
    "        self.data_paths = data_paths\n",
    "        self.crop = crop\n",
    "        \n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "        self.transforms = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            normalize\n",
    "        ])   \n",
    "        self.transforms2 = T.Compose([\n",
    "            T.ToTensor()\n",
    "        ])               \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        rgb_path = self.data_paths[index]\n",
    "\n",
    "        # crop setting\n",
    "        crop_width = 90\n",
    "        crop_height = 150\n",
    "        mid_x, mid_y = 90, 90\n",
    "        offset_x, offset_y = crop_width//2, crop_height//2\n",
    "        \n",
    "        # RGB open and crop \n",
    "        rgb_data = cv2.imread(rgb_path)\n",
    "        rgb_data = cv2.cvtColor(rgb_data, cv2.COLOR_BGR2RGB)\n",
    "        rgb_data = cv2.resize(rgb_data, (180,180), interpolation=cv2.INTER_CUBIC)\n",
    "        if self.crop == True:\n",
    "            rgb_data = rgb_data[mid_y-offset_y:mid_y+offset_y, mid_x-offset_x:mid_x+offset_x]     \n",
    "        if self.transforms is not None :\n",
    "            rgb_data = self.transforms(rgb_data)\n",
    "            \n",
    "        # label - { 0 : real , 1 : mask }\n",
    "        if 'bonafide' in rgb_path :\n",
    "            label = 0\n",
    "        elif 'attack_mask' in rgb_path :\n",
    "            label = 1\n",
    "        elif 'attack_replay' in rgb_path :\n",
    "            label = 1\n",
    "        elif 'attack_paper' in rgb_path :\n",
    "            label = 1\n",
    "\n",
    "        return rgb_data, label\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "\n",
    "def load_dataset(): \n",
    "        \n",
    "    ## Input : RGB(3-channel) + Depth(1-channel) + Point_Cloud(3-channel)\n",
    "    data_path = '/mnt/nas3/yrkim/liveness_lidar_project/GC_project/LDFAS/1. Indoor'\n",
    "    npy_path = '/mnt/nas3/yrkim/liveness_lidar_project/GC_project/LDFAS/NPY_Files/1. Indoor'\n",
    "\n",
    "    traindata_count = [1]\n",
    " \n",
    "    train_img_paths = []\n",
    "    for i in traindata_count :\n",
    "        img_path = osp.join(data_path, str(i), 'bonafide')\n",
    "        files = os.listdir(img_path)\n",
    "        rgbs = [j for j in files if (j.split('.')[-1]=='jpg') and (j.split('_')[0]=='rgb')]\n",
    "        \n",
    "        # RGB \n",
    "        bonafide_files = [osp.join(data_path, str(i), 'bonafide', j) for j in rgbs]\n",
    "        paper_files= [osp.join(data_path, str(i), 'attack_paper', j) for j in rgbs]\n",
    "        replay_files= [osp.join(data_path, str(i), 'attack_replay', j) for j in rgbs]\n",
    "        mask_files= [osp.join(data_path, str(i), 'attack_mask', j) for j in rgbs]\n",
    "\n",
    "        # bonafide\n",
    "        train_img_paths += list(bonafide_files)[:]\n",
    "        \n",
    "        # PAs\n",
    "        train_img_paths += list(paper_files)[:]\n",
    "        train_img_paths += list(replay_files)[:]\n",
    "        train_img_paths += list(mask_files)[:]\n",
    " \n",
    "    train_dataset=Face_Data(train_img_paths, False)\n",
    "\n",
    "    return train_dataset\n",
    "\n",
    "rgb_model = Face_Detection_Model(3).to('cuda:0').eval()\n",
    "\n",
    "train_dataset = load_dataset()\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "for step, data in enumerate(train_loader) :  \n",
    "    rgb, label = data\n",
    "    rgb = rgb.float().to('cuda:0') \n",
    "    label = label.float().to('cuda:0') \n",
    "\n",
    "    _ = rgb_model(rgb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Network import Face_Detection_Model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T \n",
    "import os\n",
    "import os.path as osp\n",
    "import cv2\n",
    "\n",
    "fix_device = 'cuda:0'\n",
    "\n",
    "class Face_Data(Dataset):\n",
    "\n",
    "    def __init__(self, data_paths, crop=False):\n",
    "        self.data_paths = data_paths\n",
    "        self.crop = crop\n",
    "        \n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "        self.transforms = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            normalize\n",
    "        ])   \n",
    "        self.transforms2 = T.Compose([\n",
    "            T.ToTensor()\n",
    "        ])               \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        rgb_path = self.data_paths[index]  \n",
    "             \n",
    "        # crop setting\n",
    "        crop_width = 90\n",
    "        crop_height = 150\n",
    "        mid_x, mid_y = 90, 90\n",
    "        offset_x, offset_y = crop_width//2, crop_height//2\n",
    "        \n",
    "        # RGB open and crop \n",
    "        rgb_data = cv2.imread(rgb_path)\n",
    "        rgb_data = cv2.cvtColor(rgb_data, cv2.COLOR_BGR2RGB)\n",
    "        rgb_data = cv2.resize(rgb_data, (180,180), interpolation=cv2.INTER_CUBIC)\n",
    "        if self.crop == True:\n",
    "            rgb_data = rgb_data[mid_y-offset_y:mid_y+offset_y, mid_x-offset_x:mid_x+offset_x]     \n",
    "        if self.transforms is not None :\n",
    "            rgb_data = self.transforms(rgb_data)\n",
    "            \n",
    "        # label - { 0 : real , 1 : mask }\n",
    "        if 'bonafide' in rgb_path :\n",
    "            label = 0\n",
    "        elif 'attack_mask' in rgb_path :\n",
    "            label = 1\n",
    "        elif 'attack_replay' in rgb_path :\n",
    "            label = 1\n",
    "        elif 'attack_paper' in rgb_path :\n",
    "            label = 1\n",
    "\n",
    "        return rgb_data, label\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "\n",
    "def load_dataset(): \n",
    "        \n",
    "    ## Input : RGB(3-channel) + Depth(1-channel) + Point_Cloud(3-channel)\n",
    "    data_path = '/mnt/nas3/yrkim/liveness_lidar_project/GC_project/LDFAS/1. Indoor'\n",
    "\n",
    "    traindata_count = [i for i in range(1,6)]\n",
    "    train_img_paths = []\n",
    "    for i in traindata_count :\n",
    "        img_path = osp.join(data_path, str(i), 'bonafide')\n",
    "        files = os.listdir(img_path)\n",
    "        rgbs = [j for j in files if (j.split('.')[-1]=='jpg') and (j.split('_')[0]=='rgb')]\n",
    "        \n",
    "        # RGB \n",
    "        bonafide_files = [osp.join(data_path, str(i), 'bonafide', j) for j in rgbs]\n",
    "        # paper_files= [osp.join(data_path, str(i), 'attack_paper', j) for j in rgbs]\n",
    "        # replay_files= [osp.join(data_path, str(i), 'attack_replay', j) for j in rgbs]\n",
    "        # mask_files= [osp.join(data_path, str(i), 'attack_mask', j) for j in rgbs]\n",
    "     \n",
    "        # bonafide\n",
    "        train_img_paths += list(bonafide_files)[:]\n",
    "        \n",
    "        # PAs\n",
    "        # train_img_paths += list(paper_files)[:]\n",
    "        # train_img_paths += list(replay_files)[:]\n",
    "        # train_img_paths += list(mask_files)[:]\n",
    " \n",
    "    train_dataset=Face_Data(train_img_paths, False)\n",
    "\n",
    "    return train_dataset\n",
    "\n",
    "rgb_model = Face_Detection_Model(3).to(fix_device).eval()\n",
    "\n",
    "train_dataset = load_dataset()\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "for step, data in enumerate(train_loader) :  \n",
    "    rgb, label = data\n",
    "    rgb = rgb.float().to(fix_device) \n",
    "    label = label.float().to(fix_device) \n",
    "\n",
    "    _ = rgb_model(rgb) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yrkim",
   "language": "python",
   "name": "yrkim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "612335445b59884117d59db647b17f4de1ad18c90f8d9b923a6ba1babb256810"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
